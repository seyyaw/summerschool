{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyyaw/summerschool/blob/main/NLP_III_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "09054155",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "09054155",
        "outputId": "3870a002-c271-4ebe-b55e-695b355de060"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> \n",
              "table {display: block;} \n",
              "td {\n",
              "  font-size: 18px\n",
              "}\n",
              ".rendered_html { font-size: 28px; }\n",
              "*{ line-height: 200%; }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%html\n",
        "<style>\n",
        "table {display: block;}\n",
        "td {\n",
        "  font-size: 18px\n",
        "}\n",
        ".rendered_html { font-size: 28px; }\n",
        "*{ line-height: 200%; }\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0299f05d",
      "metadata": {
        "id": "0299f05d"
      },
      "source": [
        "# Summer School for Women in AI and Data Sceince\n",
        "---\n",
        "---\n",
        "# <span style=\"color:blue\">Natural Language Processing</span>\n",
        " ---\n",
        "## -  <span style=\"color:red\">Seid Muhie Yimam  - UHH, HCDS</span>\n",
        "## -  <span style=\"color:red\">Hellina Hailu Negatu -  University of Berkeley</span>\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4861f7f",
      "metadata": {
        "id": "d4861f7f"
      },
      "source": [
        "# Content\n",
        "1. Further text processing\n",
        "1. NLP applications, ML, and Feature engineering\n",
        "1. Text representation\n",
        "   1. One hot embedding/bag of words\n",
        "   1. TFIDF\n",
        "   1. Word2vec\n",
        "   1. Transformer, BERT and RoBERTa\n",
        "1. Large Language Models\n",
        "1. Resources and repositories"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "590a1b51",
      "metadata": {
        "id": "590a1b51"
      },
      "source": [
        "# Transformers, BERT and RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9edc5e1a",
      "metadata": {
        "id": "9edc5e1a"
      },
      "source": [
        "# Transformers\n",
        "Transformers are a recent advancement in machine learning known for their exceptional ability to manage context, which allows them to generate coherent text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8616190",
      "metadata": {
        "id": "f8616190"
      },
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GIVM8Wat6Vq8W7Eff-f_5w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be327152",
      "metadata": {
        "id": "be327152"
      },
      "source": [
        "At the basic, transformers help predicting the next word based on the previous contex\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1aA6C837tO84KrwCzfmO8w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea2d4af",
      "metadata": {
        "id": "aea2d4af"
      },
      "source": [
        "Command: Write a story.\n",
        "- Response: Once\n",
        "\n",
        "Next command: Write a story. Once\n",
        "\n",
        "- Response: upon\n",
        "\n",
        "Next command: Write a story. Once upon\n",
        "- Response: a\n",
        "\n",
        "Next command: Write a story. Once upon a\n",
        "- Response: time\n",
        "\n",
        "Next command: Write a story. Once upon a time\n",
        "- Response: there"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3732592d",
      "metadata": {
        "id": "3732592d"
      },
      "source": [
        "# Transformer components\n",
        "    Tokenization\n",
        "    Embedding\n",
        "    Positional encoding\n",
        "    Transformer block (several of these)\n",
        "    Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3845efc0",
      "metadata": {
        "id": "3845efc0"
      },
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKNbZWqHGpq7LeC5k6auRA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97b80d1",
      "metadata": {
        "id": "d97b80d1"
      },
      "source": [
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zTFLvNHbbRlAMZBqPkalkA.png)\n",
        "Tokenization is the process of converting text into a series of known tokens, including words, prefixes, suffixes, and punctuation marks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f10b9cd",
      "metadata": {
        "id": "9f10b9cd"
      },
      "source": [
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsatPWp0Q2mcX5pbxijjfQ.png)\n",
        "Embedding converts tokenized input into numerical vectors, where similar texts yield similar vector values and dissimilar texts differ numerically."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5768a4",
      "metadata": {
        "id": "4f5768a4"
      },
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tY6CBYjfMDc2b-4PoZGiHA.png)\n",
        "Positional encoding adds a positional vector to each word, in order to keep track of the positions of the words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f6424a0",
      "metadata": {
        "id": "6f6424a0"
      },
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lf2bhrFFS4y--WmgBmAyjQ.png)\n",
        "A transformer block consists of an attention component and a feedforward network, crucial for contextualizing and processing sequence data in models predicting the next word in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1c9683",
      "metadata": {
        "id": "dc1c9683"
      },
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rql7F3gVwcDIAwLbWKV8kQ.png)\n",
        "The attention mechanism helps resolve contextual ambiguities in language models by adjusting how words relate based on their surrounding words. It differentiates meanings like distinguishing \"bank\" in **Money in the bank** from \"bank\" in **The bank of the river.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2567d3a",
      "metadata": {
        "id": "a2567d3a"
      },
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tW3hIJWaKuZ9bIMIiqudZA.png)\n",
        "The softmax layer in a transformer converts scores into probabilities for each word, ensuring they sum to one. This step prioritizes words with higher scores by giving them greater likelihoods of being selected as the next word. By sampling from these probabilities, the transformer determines the most likely next word, such as \"once\" from options like \"Once,\" \"Somewhere,\" and \"There.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd7f2ee",
      "metadata": {
        "id": "1dd7f2ee"
      },
      "source": [
        "# Finetuning Transformer models\n",
        "Fine-tuning is the process of training a pre-trained transformer model on a specific dataset to enhance its ability to perform particular tasks, like answering questions or functioning as a chatbot. This additional training phase helps the model adjust from broad, general knowledge to a more focused application, improving its responses and relevance. By fine-tuning, transformers can overcome biases toward their initial training and better adapt to new, task-specific data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd2df854",
      "metadata": {
        "id": "cd2df854"
      },
      "source": [
        "# Resources\n",
        "1. ![Transformers explained](https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}